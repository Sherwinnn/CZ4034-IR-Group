{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\limju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacintha's pre processing\n",
    "- Remove duplicates\n",
    "- Remove empty rows\n",
    "- Remove non english\n",
    "- Convert Emojis to english"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limju\\AppData\\Local\\Temp\\ipykernel_9504\\3744849456.py:1: DtypeWarning: Columns (62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('final_cleaned_reviews.csv') #this is entirety of reviews from jacintha\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_cleaned_reviews_J.csv') #this is entirety of reviews from jacintha, add idx to column names\n",
    "df = df[['idx','title','stars','text']]\n",
    "\n",
    "testdf = pd.read_csv('to_annotate.csv') #test reviews only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testidx = testdf.idx.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = df[~df['idx'].isin(testidx)] #train reviews only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further cleaning and preprocessing\n",
    "- This is applied to both train and test sets. It is applied to train set now, and test set in the individual model notebooks\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove stopwords\n",
    "3. Extra whitespace\n",
    "4. lemmatize (with POS)\n",
    "5. Lowercase\n",
    "6. Change contractions\n",
    "7. Remove punctuations and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_stopwords(reviews):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'not', 'is', 'but'}\n",
    "    STOPWORDS -= custom_stopwords\n",
    "    if STOPWORDS is None:\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "    # Split the reviews into words and remove stopwords\n",
    "    words = reviews.split()\n",
    "    words_filtered = [word for word in words if word not in STOPWORDS]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_reviews = ' '.join(words_filtered)\n",
    "    \n",
    "    return filtered_reviews\n",
    "\n",
    "def remove_extra_whitespace(reviews):\n",
    "    return \" \".join(reviews.split())\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tags = nltk.pos_tag(text)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "    return tags\n",
    "\n",
    "def lemmaSentence(reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_text = ''\n",
    "    tok_text = word_tokenize(reviews)\n",
    "    tags = get_wordnet_pos(tok_text)\n",
    "    for i in range(len(tok_text)):\n",
    "        lemma_text = lemma_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "    return lemma_text[1:]\n",
    "\n",
    "def lower_case(review):\n",
    "    return review.lower()\n",
    "\n",
    "# change contraction words such sa I'm = I am, shouldn't = should not\n",
    "def change_contractions(review):\n",
    "    \n",
    "    expanded_words = [contractions.fix(word) for word in review.split()]\n",
    "\n",
    "    expanded_review = ' '.join(expanded_words)\n",
    "    return expanded_review\n",
    "\n",
    "# Remove Punctuations\n",
    "def remove_punctuations(review):\n",
    "    \n",
    "    new_review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    return new_review\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(review):\n",
    "    \n",
    "    mapping = str.maketrans('', '', string.digits)\n",
    "    new_review = review.translate(mapping)\n",
    "    \n",
    "    return new_review\n",
    "\n",
    "\n",
    "def clean_text(data):\n",
    "\n",
    "    data = data.apply(lower_case)\n",
    "    data = data.apply(change_contractions)\n",
    "    data = data.apply(remove_emojis)\n",
    "    data = data.apply(remove_punctuations)\n",
    "    data = data.apply(remove_numbers)\n",
    "    data = data.apply(remove_stopwords)\n",
    "    data = data.apply(remove_extra_whitespace)\n",
    "    data = data.apply(lemmaSentence)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limju\\AppData\\Local\\Temp\\ipykernel_9504\\4121735024.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  traindf['cleaned_text'] = clean_text(traindf.text)\n"
     ]
    }
   ],
   "source": [
    "traindf['cleaned_text'] = clean_text(traindf.text)\n",
    "traindf = traindf.reset_index(drop=True)\n",
    "#testdf['cleaned_text'] = clean_text(testdf.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_csv(\"train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Polarity_Anno1</th>\n",
       "      <th>Polarity_Anno2</th>\n",
       "      <th>Polarity_Anno3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food is generally good. Serving size on the sm...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My friend and I are post duty from work and de...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their food is tasty and affordable. We ordered...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Mixed Experience at Tapas Club: Delectable C...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average taste with affordable price. Overall g...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Polarity_Anno1  \\\n",
       "0  Food is generally good. Serving size on the sm...       POSITIVE   \n",
       "1  My friend and I are post duty from work and de...       NEGATIVE   \n",
       "2  Their food is tasty and affordable. We ordered...       POSITIVE   \n",
       "3  A Mixed Experience at Tapas Club: Delectable C...       POSITIVE   \n",
       "4  Average taste with affordable price. Overall g...       POSITIVE   \n",
       "\n",
       "  Polarity_Anno2 Polarity_Anno3  \n",
       "0       POSITIVE       POSITIVE  \n",
       "1       NEGATIVE       POSITIVE  \n",
       "2       POSITIVE       POSITIVE  \n",
       "3       POSITIVE       POSITIVE  \n",
       "4       POSITIVE       POSITIVE  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_csv('test_data.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = test_df.Polarity_Anno1\n",
    "a2 = test_df.Polarity_Anno2\n",
    "a3 = test_df.Polarity_Anno3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579901511389728"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "(cohen_kappa_score(a1, a2) + cohen_kappa_score(a1, a3) +cohen_kappa_score(a2, a3))/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
