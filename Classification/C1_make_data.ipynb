{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\limju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\limju\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import wordnet\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacintha's pre processing\n",
    "- Remove duplicates\n",
    "- Remove empty rows\n",
    "- Remove non english\n",
    "- Convert Emojis to english"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limju\\AppData\\Local\\Temp\\ipykernel_16416\\335519818.py:1: DtypeWarning: Columns (62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('final_cleaned_reviews_J.csv') #this is entirety of reviews from jacintha, add idx to column names\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_cleaned_reviews_J.csv') #this is entirety of reviews from jacintha, add idx to column names\n",
    "df = df[['idx','title','stars','text']]\n",
    "\n",
    "testdf = pd.read_csv('to_annotate.csv') #test reviews only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>publishedAtDate</th>\n",
       "      <th>likesCount</th>\n",
       "      <th>name</th>\n",
       "      <th>reviewerNumberOfReviews</th>\n",
       "      <th>isLocalGuide</th>\n",
       "      <th>responseFromOwnerDate</th>\n",
       "      <th>...</th>\n",
       "      <th>reviewImageUrls/42</th>\n",
       "      <th>reviewImageUrls/43</th>\n",
       "      <th>reviewImageUrls/44</th>\n",
       "      <th>reviewImageUrls/45</th>\n",
       "      <th>reviewImageUrls/46</th>\n",
       "      <th>reviewImageUrls/47</th>\n",
       "      <th>reviewImageUrls/48</th>\n",
       "      <th>reviewImageUrls/49</th>\n",
       "      <th>lang</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7678</td>\n",
       "      <td>Iro Iro Japanese Restaurant</td>\n",
       "      <td>5</td>\n",
       "      <td>We were so excited to eat so the photos are ta...</td>\n",
       "      <td>2023-11-25T02:55:12.562Z</td>\n",
       "      <td>1</td>\n",
       "      <td>Lee Joleen</td>\n",
       "      <td>39</td>\n",
       "      <td>False</td>\n",
       "      <td>2023-12-10T15:15:19.003Z</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5117</td>\n",
       "      <td>Pura Brasa Singapore</td>\n",
       "      <td>5</td>\n",
       "      <td>Really great Spanish food at reasonable prices...</td>\n",
       "      <td>2024-02-14T16:50:26.656Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Randy Choo</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3138</td>\n",
       "      <td>Sheik Jalani Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>Food range is amazing, food equality is slight...</td>\n",
       "      <td>2018-04-19T05:10:55.527Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Miss Ain M</td>\n",
       "      <td>61</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2683</td>\n",
       "      <td>Black Society 黑社会</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice ambience - the room was well decorated wi...</td>\n",
       "      <td>2023-12-23T12:36:58.020Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Marilyn Lau</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>620</td>\n",
       "      <td>BOMBAY DINING-NORTH INDIAN RESTAURANT</td>\n",
       "      <td>2</td>\n",
       "      <td>Ordered on Deliveroo and food arrived 50min la...</td>\n",
       "      <td>2021-05-22T11:13:00.211Z</td>\n",
       "      <td>4</td>\n",
       "      <td>Owen Lam</td>\n",
       "      <td>47</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>6431</td>\n",
       "      <td>Melben Seafood Restaurant - Depot Lane</td>\n",
       "      <td>2</td>\n",
       "      <td>Purchased a mongolian style beef rice with egg...</td>\n",
       "      <td>2023-11-25T15:04:57.753Z</td>\n",
       "      <td>0</td>\n",
       "      <td>jinsen goh</td>\n",
       "      <td>48</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>2572</td>\n",
       "      <td>Tambi Restaurant</td>\n",
       "      <td>5</td>\n",
       "      <td>Oxtail Bone Marrow Murtabak 8/10 (Kimchi was a...</td>\n",
       "      <td>2024-02-21T13:37:15.730Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Layla Said</td>\n",
       "      <td>37</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>9377</td>\n",
       "      <td>Ba Shu Sichuan Restaurant</td>\n",
       "      <td>5</td>\n",
       "      <td>Great tasting and authentic dishes here.\\n\\nWe...</td>\n",
       "      <td>2022-01-31T04:32:26.426Z</td>\n",
       "      <td>5</td>\n",
       "      <td>Jonneh Ng</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>9498</td>\n",
       "      <td>Mendon Bedok Mall</td>\n",
       "      <td>5</td>\n",
       "      <td>a small cozy nook beside Tori-Q that serves re...</td>\n",
       "      <td>2023-11-05T10:49:49.070Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Xing Yi Han</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>7755</td>\n",
       "      <td>Hyderabad Bawarchi Authentic North Indian Rest...</td>\n",
       "      <td>5</td>\n",
       "      <td>Great place for Indian food.. affordable and n...</td>\n",
       "      <td>2023-08-02T11:46:53.103Z</td>\n",
       "      <td>0</td>\n",
       "      <td>lavanya veeraragavan</td>\n",
       "      <td>38</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       idx                                              title  stars  \\\n",
       "0     7678                        Iro Iro Japanese Restaurant      5   \n",
       "1     5117                               Pura Brasa Singapore      5   \n",
       "2     3138                            Sheik Jalani Restaurant      4   \n",
       "3     2683                                  Black Society 黑社会      5   \n",
       "4      620              BOMBAY DINING-NORTH INDIAN RESTAURANT      2   \n",
       "...    ...                                                ...    ...   \n",
       "1063  6431             Melben Seafood Restaurant - Depot Lane      2   \n",
       "1064  2572                                   Tambi Restaurant      5   \n",
       "1065  9377                          Ba Shu Sichuan Restaurant      5   \n",
       "1066  9498                                  Mendon Bedok Mall      5   \n",
       "1067  7755  Hyderabad Bawarchi Authentic North Indian Rest...      5   \n",
       "\n",
       "                                                   text  \\\n",
       "0     We were so excited to eat so the photos are ta...   \n",
       "1     Really great Spanish food at reasonable prices...   \n",
       "2     Food range is amazing, food equality is slight...   \n",
       "3     Nice ambience - the room was well decorated wi...   \n",
       "4     Ordered on Deliveroo and food arrived 50min la...   \n",
       "...                                                 ...   \n",
       "1063  Purchased a mongolian style beef rice with egg...   \n",
       "1064  Oxtail Bone Marrow Murtabak 8/10 (Kimchi was a...   \n",
       "1065  Great tasting and authentic dishes here.\\n\\nWe...   \n",
       "1066  a small cozy nook beside Tori-Q that serves re...   \n",
       "1067  Great place for Indian food.. affordable and n...   \n",
       "\n",
       "               publishedAtDate  likesCount                  name  \\\n",
       "0     2023-11-25T02:55:12.562Z           1            Lee Joleen   \n",
       "1     2024-02-14T16:50:26.656Z           0            Randy Choo   \n",
       "2     2018-04-19T05:10:55.527Z           0            Miss Ain M   \n",
       "3     2023-12-23T12:36:58.020Z           0           Marilyn Lau   \n",
       "4     2021-05-22T11:13:00.211Z           4              Owen Lam   \n",
       "...                        ...         ...                   ...   \n",
       "1063  2023-11-25T15:04:57.753Z           0            jinsen goh   \n",
       "1064  2024-02-21T13:37:15.730Z           0            Layla Said   \n",
       "1065  2022-01-31T04:32:26.426Z           5             Jonneh Ng   \n",
       "1066  2023-11-05T10:49:49.070Z           0           Xing Yi Han   \n",
       "1067  2023-08-02T11:46:53.103Z           0  lavanya veeraragavan   \n",
       "\n",
       "      reviewerNumberOfReviews  isLocalGuide     responseFromOwnerDate  ...  \\\n",
       "0                          39         False  2023-12-10T15:15:19.003Z  ...   \n",
       "1                          26          True                       NaN  ...   \n",
       "2                          61         False                       NaN  ...   \n",
       "3                           2         False                       NaN  ...   \n",
       "4                          47          True                       NaN  ...   \n",
       "...                       ...           ...                       ...  ...   \n",
       "1063                       48          True                       NaN  ...   \n",
       "1064                       37          True                       NaN  ...   \n",
       "1065                        8         False                       NaN  ...   \n",
       "1066                        5         False                       NaN  ...   \n",
       "1067                       38          True                       NaN  ...   \n",
       "\n",
       "      reviewImageUrls/42 reviewImageUrls/43  reviewImageUrls/44  \\\n",
       "0                    NaN                NaN                 NaN   \n",
       "1                    NaN                NaN                 NaN   \n",
       "2                    NaN                NaN                 NaN   \n",
       "3                    NaN                NaN                 NaN   \n",
       "4                    NaN                NaN                 NaN   \n",
       "...                  ...                ...                 ...   \n",
       "1063                 NaN                NaN                 NaN   \n",
       "1064                 NaN                NaN                 NaN   \n",
       "1065                 NaN                NaN                 NaN   \n",
       "1066                 NaN                NaN                 NaN   \n",
       "1067                 NaN                NaN                 NaN   \n",
       "\n",
       "     reviewImageUrls/45 reviewImageUrls/46 reviewImageUrls/47  \\\n",
       "0                   NaN                NaN                NaN   \n",
       "1                   NaN                NaN                NaN   \n",
       "2                   NaN                NaN                NaN   \n",
       "3                   NaN                NaN                NaN   \n",
       "4                   NaN                NaN                NaN   \n",
       "...                 ...                ...                ...   \n",
       "1063                NaN                NaN                NaN   \n",
       "1064                NaN                NaN                NaN   \n",
       "1065                NaN                NaN                NaN   \n",
       "1066                NaN                NaN                NaN   \n",
       "1067                NaN                NaN                NaN   \n",
       "\n",
       "     reviewImageUrls/48  reviewImageUrls/49  lang  num_words  \n",
       "0                   NaN                 NaN    en        338  \n",
       "1                   NaN                 NaN    en         40  \n",
       "2                   NaN                 NaN    en         32  \n",
       "3                   NaN                 NaN    en         46  \n",
       "4                   NaN                 NaN    en         51  \n",
       "...                 ...                 ...   ...        ...  \n",
       "1063                NaN                 NaN    en         26  \n",
       "1064                NaN                 NaN    en         57  \n",
       "1065                NaN                 NaN    en         58  \n",
       "1066                NaN                 NaN    en         90  \n",
       "1067                NaN                 NaN    en          9  \n",
       "\n",
       "[1068 rows x 80 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testidx = testdf.idx.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = df[~df['idx'].isin(testidx)] #train reviews only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further cleaning and preprocessing\n",
    "- This is applied to both train and test sets. It is applied to train set now, and test set in the individual model notebooks\n",
    "\n",
    "1. Remove emojis\n",
    "2. Remove stopwords\n",
    "3. Extra whitespace\n",
    "4. lemmatize (with POS)\n",
    "5. Lowercase\n",
    "6. Change contractions\n",
    "7. Remove punctuations and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emojis \n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "        u\"\\U00002500-\\U00002BEF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  \n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "def remove_stopwords(reviews):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    custom_stopwords = {'not', 'is', 'but'}\n",
    "    STOPWORDS -= custom_stopwords\n",
    "    if STOPWORDS is None:\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "    # Split the reviews into words and remove stopwords\n",
    "    words = reviews.split()\n",
    "    words_filtered = [word for word in words if word not in STOPWORDS]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    filtered_reviews = ' '.join(words_filtered)\n",
    "    \n",
    "    return filtered_reviews\n",
    "\n",
    "def remove_extra_whitespace(reviews):\n",
    "    return \" \".join(reviews.split())\n",
    "\n",
    "def get_wordnet_pos(text):\n",
    "    # Map POS tag to first character lemmatize() accepts\n",
    "    tags = nltk.pos_tag(text)\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    tags = [tag_dict.get(tag[1][0],  wordnet.NOUN) for tag in tags]\n",
    "    return tags\n",
    "\n",
    "def lemmaSentence(reviews):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_text = ''\n",
    "    tok_text = word_tokenize(reviews)\n",
    "    tags = get_wordnet_pos(tok_text)\n",
    "    for i in range(len(tok_text)):\n",
    "        lemma_text = lemma_text + ' ' + lemmatizer.lemmatize(tok_text[i], tags[i])\n",
    "    return lemma_text[1:]\n",
    "\n",
    "def lower_case(review):\n",
    "    return review.lower()\n",
    "\n",
    "# change contraction words such sa I'm = I am, shouldn't = should not\n",
    "def change_contractions(review):\n",
    "    \n",
    "    expanded_words = [contractions.fix(word) for word in review.split()]\n",
    "\n",
    "    expanded_review = ' '.join(expanded_words)\n",
    "    return expanded_review\n",
    "\n",
    "# Remove Punctuations\n",
    "def remove_punctuations(review):\n",
    "    \n",
    "    new_review = review.translate(str.maketrans('', '', string.punctuation))\n",
    "    return new_review\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(review):\n",
    "    \n",
    "    mapping = str.maketrans('', '', string.digits)\n",
    "    new_review = review.translate(mapping)\n",
    "    \n",
    "    return new_review\n",
    "\n",
    "\n",
    "def clean_text(data):\n",
    "\n",
    "    data = data.apply(lower_case)\n",
    "    data = data.apply(change_contractions)\n",
    "    data = data.apply(remove_emojis)\n",
    "    data = data.apply(remove_punctuations)\n",
    "    data = data.apply(remove_numbers)\n",
    "    data = data.apply(remove_stopwords)\n",
    "    data = data.apply(remove_extra_whitespace)\n",
    "    data = data.apply(lemmaSentence)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\limju\\AppData\\Local\\Temp\\ipykernel_9504\\4121735024.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  traindf['cleaned_text'] = clean_text(traindf.text)\n"
     ]
    }
   ],
   "source": [
    "traindf['cleaned_text'] = clean_text(traindf.text)\n",
    "traindf = traindf.reset_index(drop=True)\n",
    "#testdf['cleaned_text'] = clean_text(testdf.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.to_csv(\"train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Polarity_Anno1</th>\n",
       "      <th>Polarity_Anno2</th>\n",
       "      <th>Polarity_Anno3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Food is generally good. Serving size on the sm...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My friend and I are post duty from work and de...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Their food is tasty and affordable. We ordered...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A Mixed Experience at Tapas Club: Delectable C...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Average taste with affordable price. Overall g...</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Polarity_Anno1  \\\n",
       "0  Food is generally good. Serving size on the sm...       POSITIVE   \n",
       "1  My friend and I are post duty from work and de...       NEGATIVE   \n",
       "2  Their food is tasty and affordable. We ordered...       POSITIVE   \n",
       "3  A Mixed Experience at Tapas Club: Delectable C...       POSITIVE   \n",
       "4  Average taste with affordable price. Overall g...       POSITIVE   \n",
       "\n",
       "  Polarity_Anno2 Polarity_Anno3  \n",
       "0       POSITIVE       POSITIVE  \n",
       "1       NEGATIVE       POSITIVE  \n",
       "2       POSITIVE       POSITIVE  \n",
       "3       POSITIVE       POSITIVE  \n",
       "4       POSITIVE       POSITIVE  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=pd.read_csv('test_data.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anno2int(text):\n",
    "    if text=='POSITIVE':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "a1 = test_df.Polarity_Anno1.apply(anno2int)\n",
    "a2 = test_df.Polarity_Anno2.apply(anno2int)\n",
    "a3 = test_df.Polarity_Anno3.apply(anno2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8574212688270192"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.stats.inter_rater import fleiss_kappa\n",
    "from statsmodels.stats.inter_rater import aggregate_raters\n",
    "\n",
    "count_matrix = aggregate_raters(pd.DataFrame({'a1':a1, 'a2':a2, 'a3':a3}).values)\n",
    "fleiss_kappa(count_matrix[0], method='fleiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8579901511389728"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "(cohen_kappa_score(a1, a2) + cohen_kappa_score(a1, a3) +cohen_kappa_score(a2, a3))/3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
